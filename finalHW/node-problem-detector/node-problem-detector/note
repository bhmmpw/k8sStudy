## 문제의 정의
## Node 레벨에서 장애 혹은 이슈 발생 시 해당 Node를 drain하고 자동으로 scale-in 하기 위한 어플리케이션 개발 검토.
## 장애 관련하여 kernel.log 를 확인 하던 중 아래와 같은 로그를 발견
##
## kernel: [953239.600597] INFO: task dockerd:2897 blocked for more than 120 seconds.
##
## kubernetes에서 사용되는 Node Problem Detector가 있음을 확인하고 개발 대신 Node Problem Detector 적용하기로 함.
##
## Node Problem Detector
## Kubernetes를 구성하고 있는 Node의 다양한 문제에 대한 가시성을 제공하기 위해 개발되었으며, 현재 Kubernetes cluster addon으로 등록되어 있음.
##
## 현재 deliveryhero에서 지속적으로 개발중인것으로 보임.
##
## 동작 방식
## DaemonSet형식으로 배포되며, 필요시 standalone형태로 동작이 가능함.
## GCE 클러스터의 경우 기본 addon으로 활성화 되어 있음. (현재 과제는 GCE가 아니라서 고려하지 않음)
##
## 구성
## Problem Daemon - node-problem-detector의 서브 데몬으로 이슈를 모니터링 하여 node-problem-detector로 전송
## node-problem-detector - Problem Daemon에서 수집한 정보를 exporter를 통해 외부로 전달
## Exporter - 외부 백엔드로 데이터를 전달하기 위한 모듈로 기본적으로는 Kubernetes Exporter, Prometheus Exporter, Stackdriver Exporter가 있으며, 정의된 interface를 구현하여 exporter를 추가 가능(goLang)
# reference : https://gist.github.com/StevenACoffman/120bdbe8506e45bccc79bc73187c00bc
# reference : https://github.com/kubernetes/node-problem-detector

---
#original node-problem-detector.yaml
apiVersion: v1
data:
  kernel-monitor.json: |
    {
        "plugin": "kmsg",
        "logPath": "/dev/kmsg",
        "lookback": "5m",
        "bufferSize": 10,
        "source": "kernel-monitor",
        "conditions": [
            {
                "type": "KernelDeadlock",
                "reason": "KernelHasNoDeadlock",
                "message": "kernel has no deadlock"
            },
            {
                "type": "ReadonlyFilesystem",
                "reason": "FilesystemIsReadOnly",
                "message": "Filesystem is read-only"
            }
        ],
        "rules": [
            {
                "type": "temporary",
                "reason": "OOMKilling",
                "pattern": "Kill process \\d+ (.+) score \\d+ or sacrifice child\\nKilled process \\d+ (.+) total-vm:\\d+kB, anon-rss:\\d+kB, file-rss:\\d+kB.*"
            },
            {
                "type": "temporary",
                "reason": "TaskHung",
                "pattern": "task \\S+:\\w+ blocked for more than \\w+ seconds\\."
            },
            {
                "type": "temporary",
                "reason": "UnregisterNetDevice",
                "pattern": "unregister_netdevice: waiting for \\w+ to become free. Usage count = \\d+"
            },
            {
                "type": "temporary",
                "reason": "KernelOops",
                "pattern": "BUG: unable to handle kernel NULL pointer dereference at .*"
            },
            {
                "type": "temporary",
                "reason": "KernelOops",
                "pattern": "divide error: 0000 \\[#\\d+\\] SMP"
            },
            {
                "type": "permanent",
                "condition": "KernelDeadlock",
                "reason": "AUFSUmountHung",
                "pattern": "task umount\\.aufs:\\w+ blocked for more than \\w+ seconds\\."
            },
            {
                "type": "permanent",
                "condition": "KernelDeadlock",
                "reason": "DockerHung",
                "pattern": "task docker:\\w+ blocked for more than \\w+ seconds\\."
            },
            {
                "type": "permanent",
                "condition": "ReadonlyFilesystem",
                "reason": "FilesystemIsReadOnly",
                "pattern": "Remounting filesystem read-only"
            }
        ]
    }
  docker-monitor.json: |
    {
        "plugin": "journald",
        "pluginConfig": {
            "source": "dockerd"
        },
        "logPath": "/var/log/journal",
        "lookback": "5m",
        "bufferSize": 10,
        "source": "docker-monitor",
        "conditions": [],
        "rules": [
            {
                "type": "temporary",
                "reason": "CorruptDockerImage",
                "pattern": "Error trying v2 registry: failed to register layer: rename /var/lib/docker/image/(.+) /var/lib/docker/image/(.+): directory not empty.*"
            }
        ]
    }
kind: ConfigMap
metadata:
  name: node-problem-detector-config
  namespace: kube-system


$ diff node-problem-detector.yaml my-node-problem-detector.yaml
< apiVersion:v1
< data:
< kernel-monitor.json:|
< {
< "plugin":"kmsg",
< "logPath":"/dev/kmsg",
< "lookback":"5m",
< "bufferSize":10,
< "source":"kernel-monitor",
< "conditions":[
< {
< "type":"KernelDeadlock",
< "reason":"KernelHasNoDeadlock",
< "message":"kernelhasnodeadlock"
< },
< {
< "type":"ReadonlyFilesystem",
< "reason":"FilesystemIsReadOnly",
< "message":"Filesystemisread-only"
< }
< ],
< "rules":[
< {
< "type":"temporary",
< "reason":"OOMKilling",
< "pattern":"Killprocess\\d+(.+)score\\d+orsacrificechild\\nKilledprocess\\d+(.+)total-vm:\\d+kB,anon-rss:\\d+kB,file-rss:\\d+kB.*"
< },
< {
< "type":"temporary",
< "reason":"TaskHung",
< "pattern":"task\\S+:\\w+blockedformorethan\\w+seconds\\."
< },
< {
< "type":"temporary",
< "reason":"UnregisterNetDevice",
< "pattern":"unregister_netdevice:waitingfor\\w+tobecomefree.Usagecount=\\d+"
< },
< {
< "type":"temporary",
< "reason":"KernelOops",
< "pattern":"BUG:unabletohandlekernelNULLpointerdereferenceat.*"
< },
< {
< "type":"temporary",
< "reason":"KernelOops",
< "pattern":"divideerror:0000\\[#\\d+\\]SMP"
< },
< {
< "type":"permanent",
< "condition":"KernelDeadlock",
< "reason":"AUFSUmountHung",
< "pattern":"taskumount\\.aufs:\\w+blockedformorethan\\w+seconds\\."
< },
< {
< "type":"permanent",
< "condition":"KernelDeadlock",
< "reason":"DockerHung",
< "pattern":"taskdocker:\\w+blockedformorethan\\w+seconds\\."
< },
< {
< "type":"permanent",
< "condition":"ReadonlyFilesystem",
< "reason":"FilesystemIsReadOnly",
< "pattern":"Remountingfilesystemread-only"
< }
< ]
< }
< docker-monitor.json:|
< {
< "plugin":"journald",
< "pluginConfig":{
< "source":"dockerd"
< },
< "logPath":"/var/log/journal",
< "lookback":"5m",
< "bufferSize":10,
< "source":"docker-monitor",
< "conditions":[],
< "rules":[
< {
< "type":"temporary",
< "reason":"CorruptDockerImage",
< "pattern":"Errortryingv2registry:failedtoregisterlayer:rename/var/lib/docker/image/(.+)/var/lib/docker/image/(.+):directorynotempty.*"
< }
< ]
< }
< kind:ConfigMap
< metadata:
< name:node-problem-detector-config
< namespace:kube-system
---
> settings:
> log_monitors:
> -/custom-config/kernel-monitor.json
> #settings.prometheus_port--Prometheusexporterport
> prometheus_port:0
> custom_monitor_definitions:
> kernel-monitor.json:|
> {
> "plugin":"kmsg",
> "logPath":"/dev/kmsg",
> "lookback":"5m",
> "bufferSize":10,
> "source":"kernel-monitor",
> "conditions":[
> {
> "type":"KernelDeadlock",
> "reason":"KernelHasNoDeadlock",
> "message":"kernelhasnodeadlock"
> },
> {
> "type":"ReadonlyFilesystem",
> "reason":"FilesystemIsNotReadOnly",
> "message":"Filesystemisnotread-only"
> },
> {
> "type":"EFSConnectFail",
> "reason":"EFSConnectSuccess",
> "message":"EFSismountedsuccessfully"
> }
> ],
> "rules":[
> {
> "type":"temporary",
> "reason":"OOMKilling",
> "pattern":"Killprocess\\d+(.+)score\\d+orsacrificechild\\nKilledprocess\\d+(.+)total-vm:\\d+kB,anon-rss:\\d+kB,file-rss:\\d+kB.*"
> },
> {
> "type":"temporary",
> "reason":"TaskHung",
> "pattern":"task\\S+:\\w+blockedformorethan\\w+seconds\\."
> },
> {
> "type":"temporary",
> "reason":"UnregisterNetDevice",
> "pattern":"unregister_netdevice:waitingfor\\w+tobecomefree.Usagecount=\\d+"
> },
> {
> "type":"temporary",
> "reason":"KernelOops",
> "pattern":"BUG:unabletohandlekernelNULLpointerdereferenceat.*"
> },
> {
> "type":"temporary",
> "reason":"KernelOops",
> "pattern":"divideerror:0000\\[#\\d+\\]SMP"
> },
> {
> "type":"temporary",
> "reason":"MemoryReadError",
> "pattern":"CEmemoryreaderror.*"
> },
> {
> "type":"permanent",
> "condition":"KernelDeadlock",
> "reason":"AUFSUmountHung",
> "pattern":"taskumount\\.aufs:\\w+blockedformorethan\\w+seconds\\."
> },
> {
> "type":"permanent",
> "condition":"KernelDeadlock",
> "reason":"DockerHung",
> "pattern":"taskdocker:\\w+blockedformorethan\\w+seconds\\."
> },
> {
> "type":"permanent",
> "condition":"ReadonlyFilesystem",
> "reason":"FilesystemIsReadOnly",
> "pattern":"Remountingfilesystemread-only"
> },
> {
> "type":"permanent",
> "condition":"EFSConnectFail",
> "reason":"EFSConnectFail",
> "pattern":"nfs:server\\S+notresponding,timedout"
> }
> ]
> }


$ cat <<EOF > my-node-problem-detector.yaml
npd-config.yaml
settings:
  log_monitors:
  - /custom-config/kernel-monitor.json
  # settings.prometheus_port -- Prometheus exporter port
  prometheus_port: 0
  custom_monitor_definitions:
    kernel-monitor.json: |
      {
          "plugin": "kmsg",
          "logPath": "/dev/kmsg",
          "lookback": "5m",
          "bufferSize": 10,
          "source": "kernel-monitor",
          "conditions": [
              {
                  "type": "KernelDeadlock",
                  "reason": "KernelHasNoDeadlock",
                  "message": "kernel has no deadlock"
              },
              {
                  "type": "ReadonlyFilesystem",
                  "reason": "FilesystemIsNotReadOnly",
                  "message": "Filesystem is not read-only"
              },
              {
                  "type": "EFSConnectFail",
                  "reason": "EFSConnectSuccess",
                  "message": "EFS is mounted successfully"
              }
          ],
          "rules": [
              {
                  "type": "temporary",
                  "reason": "OOMKilling",
                  "pattern": "Kill process \\d+ (.+) score \\d+ or sacrifice child\\nKilled process \\d+ (.+) total-vm:\\d+kB, anon-rss:\\d+kB, file-rss:\\d+kB.*"
              },
              {
                  "type": "temporary",
                  "reason": "TaskHung",
                  "pattern": "task \\S+:\\w+ blocked for more than \\w+ seconds\\."
              },
              {
                  "type": "temporary",
                  "reason": "UnregisterNetDevice",
                  "pattern": "unregister_netdevice: waiting for \\w+ to become free. Usage count = \\d+"
              },
              {
                  "type": "temporary",
                  "reason": "KernelOops",
                  "pattern": "BUG: unable to handle kernel NULL pointer dereference at .*"
              },
              {
                  "type": "temporary",
                  "reason": "KernelOops",
                  "pattern": "divide error: 0000 \\[#\\d+\\] SMP"
              },
              {
                  "type": "temporary",
                  "reason": "MemoryReadError",
                  "pattern": "CE memory read error .*"
              },
              {
                  "type": "permanent",
                  "condition": "KernelDeadlock",
                  "reason": "AUFSUmountHung",
                  "pattern": "task umount\\.aufs:\\w+ blocked for more than \\w+ seconds\\."
              },
              {
                  "type": "permanent",
                  "condition": "KernelDeadlock",
                  "reason": "DockerHung",
                  "pattern": "task docker:\\w+ blocked for more than \\w+ seconds\\."
              },
              {
                  "type": "permanent",
                  "condition": "ReadonlyFilesystem",
                  "reason": "FilesystemIsReadOnly",
                  "pattern": "Remounting filesystem read-only"
              },
              {
                  "type": "permanent",
                  "condition": "EFSConnectFail",
                  "reason": "EFSConnectFail",
                  "pattern": "nfs: server \\S+ not responding, timed out"
              }
          ]
      }
EOF

# 사용 설명을 통한 이해
# 사용방법
# /dev/kmsg, /var/log/kern.log, systemd 등을 모니터링 할 수 있으며, 추가적인 플러그인을 통해 모니터링이 가능함.
# 예를 들어서 dead lock 상황은 아래의 이벤트 정의로 모니터링이 가능.
 {
  "type": "permanent",
   "condition": "KernelDeadlock",
   "reason": "DockerHung",
   "pattern": "task docker:\\w+ blocked for more than \\w+ seconds\\."
  }

## DrainO
## Node의 레이블 혹은 condition 값에 따라 자동으로 Node drain을 수행
## 기본적으로 Node Problem Detector 와 Cluster AutoScaler와 함께 사용
## DrainO는 Deployment로 배포되며 배포 시점에 자동으로 drain 하고자 하는 node 및 condition을 선택할 수 있음
## 대상 노드 선택 방법 ; 레이블을 이용, 특정 값 혹은 패턴 이용가능 (--node-label 혹은 --node-label-expr 옵션 이용)
## 대상 컨디션 선택 방법 : 파라메터로 대상 condition을 나열하면 해당 condition이 True가 될 경우 즉시 작업 수행
## cordon은 즉시 수행되며, drain은 설정된 delay이후에 실행됨

- command: [/draino, --dry-run, --node-label=draino-enabled=ture, Badcondition, ReallyBadCondition]

---

## Node Problem Detector Helm
$ helm repo add deliveryhero https://charts.deliveryhero.io/
$ helm install -f my-node-problem-detector-config.yaml node-problem-detector deliveryhero/node-problem-detector

## DrainO
$ cat <<EOF > my-draino.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels: {component: draino}
  name: draino
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels: {component: draino}
  name: draino
rules:
- apiGroups: ['']
  resources: [events]
  verbs: [create, patch, update]
- apiGroups: ['']
  resources: [nodes]
  verbs: [get, watch, list, update]
- apiGroups: ['']
  resources: [nodes/status]
  verbs: [patch]
- apiGroups: ['']
  resources: [pods]
  verbs: [get, watch, list]
- apiGroups: ['']
  resources: [pods/eviction]
  verbs: [create]
- apiGroups: [extensions]
  resources: [daemonsets]
  verbs: [get, watch, list]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels: {component: draino}
  name: draino
roleRef: {apiGroup: rbac.authorization.k8s.io, kind: ClusterRole, name: draino}
subjects:
- {kind: ServiceAccount, name: draino, namespace: kube-system}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels: {component: draino}
  name: draino
  namespace: kube-system
spec:
  # Draino does not currently support locking/master election, so you should
  # only run one draino at a time. Draino won't start draining nodes immediately
  # so it's usually safe for multiple drainos to exist for a brief period of
  # time.
  replicas: 1
  selector:
    matchLabels: {component: draino}
  template:
    metadata:
      labels: {component: draino}
      name: draino
      namespace: kube-system
    spec:
      containers:
      # You'll want to change these labels and conditions to suit your deployment.
      - command: [/draino, --node-label=draino-enabled=true, --evict-unreplicated-pods, --evict-emptydir-pods, --evict-daemonset-pods, EFSConnectFail, KernelDeadlock]
        image: planetlabs/draino:5e07e93
        livenessProbe:
          httpGet: {path: /healthz, port: 10002}
          initialDelaySeconds: 30
        name: draino
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
EOF

$ helm install -f my-node-problem-detector.yaml node-problem-detector deliveryhero/node-problem-detector
$ kubectl --namespace=default get pods -l "app.kubernetes.io/name=node-problem-detector,app.kubernetes.io/instance=node-problem-detector"
NAME                          READY   STATUS        RESTARTS   AGE
node-problem-detector-jpw2g   0/1     Terminating   0          58m
node-problem-detector-v2bjd   0/1     Pending       0          16m
node-problem-detector-2bcwv   1/1     Running       0          16m

$ kubectl apply -f my-draino.yaml
$ kubectl get deploy -n kube-system
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
metrics-server           1/1     1            1           4d1h
coredns                  1/1     1            1           18d
kube-state-metrics       0/1     1            0           24h
local-path-provisioner   1/1     1            1           18d
draino                   0/1     1            0           27m

$ kubectl get pod -n kube-system
get pod -n kube-system
NAME                                     READY   STATUS             RESTARTS   AGE
metrics-server-6cd998ff87-bv4hv          1/1     Running            6          4d1h
coredns-8655855d6-h76c9                  1/1     Running            10         18d
kube-state-metrics-7ffcdb8cfc-6lrv2      1/1     Running            0          24h
local-path-provisioner-6d59f47c7-p5rv9   1/1     Running            12         18d
draino-cfbf48b49-tq6wf                   0/1     CrashLoopBackOff   3          28m

# 과제 시간을 통해 적용해 보는 것은 최종적으로 CrashLoopBackOff 로 실패
# Troubleshooting 필요
# 
